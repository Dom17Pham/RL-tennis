{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning with Atari Tennis\n",
    "The aim of this notebook is to experiment with Reinforcement Learning (RL) by exploring its application in mastering the classic Atari game, Tennis.\n",
    "\n",
    "The goal is to develop and train an RL agent capable of playing Atari Tennis at a proficient level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from ale_py import ALEInterface\n",
    "from ale_py.roms import Tennis\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import save_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the ROM for Atari Tennis\n",
    "In this section, we will load the ROM (Read-Only Memory) for the Atari Tennis game, which we will use to train our Reinforcement Learning (RL) model. The Atari Learning Environment (ALE) provides a convenient interface to interact with Atari 2600 games, allowing us to simulate the game environment and retrieve observations and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ROM for Atari Tennis\n",
    "ale = ALEInterface()\n",
    "ale.loadROM(Tennis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters \n",
    "In this section, we define the key hyperparameters and training parameters that will guide the learning process of our RL agent. These parameters play a crucial role in determining the efficiency and effectiveness of the training.\n",
    "\n",
    "Hyperparameters are the variables that control the learning process. Fine-tuning these parameters can significantly impact the performance of the RL agent. Below are the hyperparameters used in this project:\n",
    "* `learning_rate` - The learning rate determines how much new information overrides the old information.\n",
    "* `exploration_factor` - The exploration factor dictates the probability of the agent exploring new actions versus exploiting known actions.\n",
    "* `exploration_factor_min` - Minimum probability of exploration.\n",
    "* `exploration_decay` - Rate at which the exploration factor decreases over time.\n",
    "* `discount_factor` - The discount factor represents the importance of future rewards.\n",
    "* `training_episodes` - The number of episodes during which the agent will interact with the environment to learn and improve its performance.\n",
    "* `testing_episodes` - The number of episodes used to evaluate the performance of the trained agent without further learning.\n",
    "* `max_steps` - The maximum number of steps the agent can take in a single episode. This prevents episodes from running indefinitely and ensures consistent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "hyperparameters = {\n",
    "    \"learning_rate\": 0.1,           # Learning Rate\n",
    "    \"exploration_factor\": 1.0,      # Exploration Factor (initial)\n",
    "    \"exploration_factor_min\":0.1,   # Minimum Exploration Factor\n",
    "    \"exploration_decay\":0.995,      # Exploration Decay\n",
    "    \"discount_factor\": 0.99,        # Discount Factor\n",
    "    \"training_episodes\": 10,        # Number of training episodes\n",
    "    \"testing_episodes\": 1,          # Number of testing episdoes\n",
    "    \"max_steps\": 100,               # Maximum steps per episode\n",
    "    \"replay_buffer_size\": 10000,    # Capacity of the replay buffer\n",
    "    \"batch_size\": 32,               # Size of batch to start sampling from \n",
    "    \"save_interval\": 100,           # Interval to save mmodel during training\n",
    "    \"priority_exponent\": 0.6,\n",
    "    \"importance_sampling\": 0.4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Environment\n",
    "In this section, we initialize the game environment for our RL agent using the Gym library, which provides a standard API for interfacing with various environments including Atari games. Specifically, we use the 'ALE/Tennis-v5' environment, which simulates the Atari Tennis game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [ 82, 126,  45],\n",
       "         [ 82, 126,  45],\n",
       "         [ 82, 126,  45]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [ 82, 126,  45],\n",
       "         [ 82, 126,  45],\n",
       "         [ 82, 126,  45]],\n",
       " \n",
       "        [[  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         [  0,   0,   0],\n",
       "         ...,\n",
       "         [ 82, 126,  45],\n",
       "         [ 82, 126,  45],\n",
       "         [ 82, 126,  45]]], dtype=uint8),\n",
       " {'lives': 0, 'episode_frame_number': 0, 'frame_number': 0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the environment\n",
    "env = gym.make('ALE/Tennis-v5',full_action_space=True)\n",
    "env.reset() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Space and Observation Space\n",
    "By analyzing the action and observation spaces, we gain valuable insights into the complexity and structure of the Atari Tennis environment. The action space informs us about the available actions that the agent can choose from, while the observation space describes the format of the information provided to the agent about the current state of the game.\n",
    "\n",
    "Understanding these spaces allows us to design RL algorithms and neural network architectures tailored to the specific characteristics of the environment, ultimately facilitating effective learning and decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = env.action_space.n \n",
    "height, width, channels = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network Model\n",
    "\n",
    "We define a function `build_model` that constructs a neural network model follows a typical architecture for deep Q-networks (DQN) used in Atari game playing agents. It is designed to process raw pixel inputs from the game environment and output Q-values for each possible action. This model will serve as the backbone for our RL agent, allowing it to learn and make decisions based on the game's observations.\n",
    "\n",
    "Function parameters:\n",
    "* `height` - Height of the input image.\n",
    "* `width` - Width of the input image.\n",
    "* `channels` - Number of color channels in the input image.\n",
    "* `actions` - Number of possible actions in the environment.\n",
    "\n",
    "Model Architecture:\n",
    "* `Convolutional Layers` - These layers are designed to extract features from the input image. Uses ReLU activation functions.\n",
    "* `Flatten ` - The output of the convolutional layers is flattened to prepare it for input to the fully connected layers.\n",
    "* `Fully Connected Layers` - These layers (Dense) with ReLU activation functions aims to process the extracted features and learn representations relevant for decision-making.\n",
    "* `Output Layer` - A fully connected layer with a linear activation function, producing the Q-values for each action in the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(height,width,channels)))\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(actions,activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">39</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22528</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">11,534,848</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">18</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,626</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m51\u001b[0m, \u001b[38;5;34m39\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m6,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m18\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m32,832\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m22528\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m11,534,848\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m131,328\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m18\u001b[0m)             │         \u001b[38;5;34m4,626\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,746,738</span> (44.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,746,738\u001b[0m (44.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,746,738</span> (44.81 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,746,738\u001b[0m (44.81 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_model(height, width, channels, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, size, alpha=0.6):\n",
    "        self.size = size\n",
    "        self.alpha = alpha\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.priorities = deque(maxlen=size)\n",
    "    \n",
    "    def add(self, experience, error):\n",
    "        priority = (error + 1e-5) ** self.alpha\n",
    "        self.buffer.append(experience)\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        scaled_priorities = np.array(self.priorities) ** beta\n",
    "\n",
    "        # Avoid NaN values\n",
    "        if np.any(np.isnan(scaled_priorities)):\n",
    "            scaled_priorities = np.nan_to_num(scaled_priorities, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        sample_probs = scaled_priorities / sum(scaled_priorities)\n",
    "        \n",
    "        # Avoid NaN values in sample_probs\n",
    "        if np.any(np.isnan(sample_probs)):\n",
    "            sample_probs = np.nan_to_num(sample_probs, nan=1.0/len(sample_probs))\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=sample_probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        total = len(self.buffer)\n",
    "        weights = (total * sample_probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for idx, error in zip(indices, errors):\n",
    "            priority = (error + 1e-5) ** self.alpha\n",
    "            self.priorities[idx] = priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "class DQNAgent:\n",
    "    def __init__(self, model, actions, hyperparameters):\n",
    "        self.model = model\n",
    "        self.actions = actions\n",
    "        self.learning_rate = hyperparameters[\"learning_rate\"]\n",
    "        self.epsilon = hyperparameters[\"exploration_factor\"]\n",
    "        self.epsilon_min = hyperparameters[\"exploration_factor_min\"]\n",
    "        self.epsilon_decay = hyperparameters[\"exploration_decay\"]\n",
    "        self.optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        self.model.compile(optimizer=self.optimizer, loss='mse')\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(hyperparameters[\"replay_buffer_size\"])\n",
    "        self.batch_size = hyperparameters[\"batch_size\"]\n",
    "        self.discount_factor = hyperparameters[\"discount_factor\"]\n",
    "        self.priority_exponent = hyperparameters[\"priority_exponent\"]\n",
    "        self.importance_sampling = hyperparameters[\"importance_sampling\"]\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            q_values = self.model.predict(state,verbose=0)\n",
    "            return np.argmax(q_values[0])\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        q_values = self.model.predict(state, verbose=0)\n",
    "        next_q_values = self.model.predict(next_state, verbose=0)\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = reward + self.discount_factor * np.max(next_q_values[0])\n",
    "        error = abs(target - q_values[0][action])\n",
    "        self.replay_buffer.add((state, action, reward, next_state, done), error)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        minibatch, indices, weights = self.replay_buffer.sample(self.batch_size, self.importance_sampling)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "        q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.model.predict(next_states, verbose=0)\n",
    "\n",
    "        targets = q_values.copy()\n",
    "        errors = np.zeros(self.batch_size)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            target = rewards[i]\n",
    "            if not dones[i]:\n",
    "                target = rewards[i] + self.discount_factor * np.max(next_q_values[i])\n",
    "            errors[i] = abs(target - q_values[i][actions[i]])\n",
    "            targets[i][actions[i]] = target\n",
    "\n",
    "        self.replay_buffer.update_priorities(indices, errors)\n",
    "        self.model.fit(states, targets, sample_weight=weights, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "def build_agent(model,actions,hyperparameters):\n",
    "    agent = DQNAgent(model=model, actions=actions,hyperparameters=hyperparameters)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = build_agent(model,actions,hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Episode 1 finished ---\n",
      "--- Episode 2 finished ---\n",
      "--- Episode 3 finished ---\n",
      "--- Episode 4 finished ---\n",
      "--- Episode 5 finished ---\n",
      "--- Episode 6 finished ---\n",
      "--- Episode 7 finished ---\n",
      "--- Episode 8 finished ---\n",
      "--- Episode 9 finished ---\n",
      "--- Episode 10 finished ---\n",
      "Training Completed in 0 hours, 25 minutes, 54 seconds\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "performance_record = []  # List to store performance of each episode\n",
    "best_average_reward = -np.inf  \n",
    "average_reward = 0\n",
    "best_model_filename = None\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(hyperparameters[\"training_episodes\"]):\n",
    "    \n",
    "    state_tuple = env.reset()\n",
    "    observation = state_tuple[0]  # Extract the observation from the tuple\n",
    "    state = np.expand_dims(observation, axis=0)  # Add batch dimension\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        observations = env.step(action)\n",
    "        next_state = observations[0]\n",
    "        reward = observations[1]\n",
    "        done = observations[2]\n",
    "        next_state = np.expand_dims(next_state, axis=0)  # Add batch dimension\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    agent.replay()\n",
    "    performance_record.append(total_reward)\n",
    "\n",
    "    # Save the model weights every save_interval episodes\n",
    "    if (episode + 1) % hyperparameters[\"save_interval\"] == 0:\n",
    "        model_filename = f\"model_episode_{episode + 1}.keras\"\n",
    "        model.save(model_filename)\n",
    "    \n",
    "    # Track the best performing model\n",
    "    average_reward = np.mean(performance_record[-100:])\n",
    "    if average_reward > best_average_reward:\n",
    "        best_average_reward = average_reward\n",
    "        model.save(\"models/best_model.keras\")\n",
    "\n",
    "    print(f\"--- Episode {episode + 1} finished ---\")\n",
    "    \n",
    "end_time = time.time()\n",
    "training_duration = end_time - start_time\n",
    "hours, rem = divmod(training_duration, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "\n",
    "print(f\"Training Completed in {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(18, 6))\n",
    "# plt.plot(range(1, hyperparameters[\"training_episodes\"] + 1), performance_record)\n",
    "# plt.xlabel('Episode')\n",
    "# plt.xticks(range(1, hyperparameters[\"training_episodes\"] + 1, 1))\n",
    "# plt.ylabel('Total Reward')\n",
    "# plt.yticks(range(int(min(performance_record)), int(max(performance_record)) + 1, 1))\n",
    "# plt.title('Performance of the RL Agent')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\RL-tennis\\venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:335: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: -24.0\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "saved_model_path = \"models/best_model.keras\"  \n",
    "loaded_model = load_model(saved_model_path)\n",
    "\n",
    "# Create a new agent with the loaded model\n",
    "testing_agent = build_agent(loaded_model,actions,hyperparameters)\n",
    "\n",
    "env = gym.make('ALE/Tennis-v5',full_action_space=True, render_mode=\"human\")\n",
    "env.reset() \n",
    "\n",
    "# Testing loop\n",
    "for episode in range(hyperparameters[\"testing_episodes\"]):\n",
    "    state_tuple = env.reset()\n",
    "    observation = state_tuple[0]  # Extract the observation from the tuple\n",
    "    state = np.expand_dims(observation, axis=0)  # Add batch dimension\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = testing_agent.act(state)\n",
    "        observations = env.step(action)\n",
    "        next_state = observations[0]\n",
    "        reward = observations[1]\n",
    "        done = observations[2]\n",
    "        next_state = np.expand_dims(next_state, axis=0)  # Add batch dimension\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomized Algorithm (Low Performance) -- For Comparison Purposes\n",
    "\n",
    "# env = gym.make('ALE/Tennis-v5',full_action_space=True,render_mode='human')\n",
    "# env.reset() \n",
    "# def random_agent(hyperparameters):\n",
    "#     episodes = hyperparameters[\"training_episodes\"]\n",
    "\n",
    "#     for episode in range(1, episodes+1):\n",
    "#         state = env.reset()\n",
    "#         done = False\n",
    "#         score = 0 \n",
    "\n",
    "#         while not done:\n",
    "#             env.render()\n",
    "#             action = env.action_space.sample()\n",
    "#             observations = env.step(action)\n",
    "#             next_state = observations[0]\n",
    "#             reward = observations[1]\n",
    "#             done = observations[2]\n",
    "#             score+=reward\n",
    "#         print('Episode:{} Score:{}'.format(episode, score))\n",
    "#     env.close()\n",
    "\n",
    "# random_agent(hyperparameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
